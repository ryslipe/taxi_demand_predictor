{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the configuration file we created called **config.py** so that we can access the HOPSWORKS variables such as the project name, feature group name, feature group version and API key. We do this so that we do not have to keep identifying them over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are fetching data in this notebook and doing it every hour, we need to keep track of the current time and the range of dates that we would like to fetch data from. This will fetch the last 28 days every hour. This is not necessary but the redundancy will help if a certain fetch does not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_date=Timestamp('2024-09-25 19:00:00')\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "current_date = pd.to_datetime(datetime.now(timezone.utc)).floor('H')\n",
    "current_date = current_date.replace(tzinfo=None)\n",
    "print(f'{current_date=}')\n",
    "\n",
    "\n",
    "# fetch data to the current time (rounded to nearest hour) from 28 days ago\n",
    "fetch_data_to = current_date\n",
    "fetch_data_from = current_date - timedelta(days = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-08-28 19:00:00')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_data_from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we do not have access to the NYC Taxi Data Warehouse (we cannot get the new rides every hour). For this purpose we will simulate a call which will show how it would work. Since we have access to the rides from a year ago we will pretend that this is the most recent demand that we can fetch. This function will load in data from exactly one year ago and get data for 28 days prior. This data is the simulated new data. We pretend that we run this code every hour to get the newest 28 days worth of data but since our data comes from a source that does not update every hour, we use this synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import load_raw_data\n",
    "\n",
    "def fetch_batch_raw_data(from_date: datetime, to_date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate production data by sampling historical data from 52 weeks ago (i.e. 1 year)\n",
    "    \"\"\"\n",
    "    # get data from a year ago from today\n",
    "    from_date_ = from_date - timedelta(days=7*52)\n",
    "    # get the data to the date which is 28 days before from_date_\n",
    "    to_date_ = to_date - timedelta(days=7*52)\n",
    "    print(f'{from_date_=}, {to_date_=}')\n",
    "\n",
    "    # download 2 files from website using our load_raw_data_function\n",
    "    rides = load_raw_data(year=from_date_.year, months=from_date_.month)\n",
    "    rides = rides[rides['pickup_datetime'] >= from_date_]\n",
    "    \n",
    "    rides_2 = load_raw_data(year=to_date_.year, months=to_date_.month)\n",
    "    rides_2 = rides_2[rides_2['pickup_datetime'] < to_date_]\n",
    "\n",
    "    rides = pd.concat([rides, rides_2])\n",
    "\n",
    "    # shift the data to pretend this is recent data - add a year to each row\n",
    "    rides['pickup_datetime'] += timedelta(days=7*52)\n",
    "\n",
    "    rides.sort_values(by=['pickup_location_id', 'pickup_datetime'], inplace=True)\n",
    "\n",
    "    return rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_date_=Timestamp('2023-08-30 19:00:00'), to_date_=Timestamp('2023-09-27 19:00:00')\n",
      "File 2023-08 was already in local storage\n",
      "File 2023-09 was already in local storage\n"
     ]
    }
   ],
   "source": [
    "# get rides\n",
    "rides = fetch_batch_raw_data(from_date=fetch_data_from, to_date=fetch_data_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can get a sense of what happened. We called the data warehouse to get 28 days worth of data from today. Since we didnt have the data we used last years data and shifted it to be 28 days ago from today. This is why the **rides** dataframe starts on 08-28-2024 (28 days ago), and ends today. All we did is retrieve 28 days worth of data but since we didnt have it we pretended to have it by creating it. Don't overthink this step, its just getting 28 days worth of *new* data. In practice we would have the real data available to us.\n",
    "\n",
    "At the time of writing this code - 09.25.2024 the newest data available is from July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2639768</th>\n",
       "      <td>2024-08-28 21:50:04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656234</th>\n",
       "      <td>2024-08-29 07:55:22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658958</th>\n",
       "      <td>2024-08-29 08:41:05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695049</th>\n",
       "      <td>2024-08-29 16:24:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702871</th>\n",
       "      <td>2024-08-29 17:15:21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2824574</th>\n",
       "      <td>2024-09-25 18:09:07</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2824595</th>\n",
       "      <td>2024-09-25 18:09:10</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341399</th>\n",
       "      <td>2024-09-25 18:22:50</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345555</th>\n",
       "      <td>2024-09-25 18:35:37</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345556</th>\n",
       "      <td>2024-09-25 18:37:00</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2580093 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickup_datetime  pickup_location_id\n",
       "2639768 2024-08-28 21:50:04                   1\n",
       "2656234 2024-08-29 07:55:22                   1\n",
       "2658958 2024-08-29 08:41:05                   1\n",
       "2695049 2024-08-29 16:24:00                   1\n",
       "2702871 2024-08-29 17:15:21                   1\n",
       "...                     ...                 ...\n",
       "2824574 2024-09-25 18:09:07                 265\n",
       "2824595 2024-09-25 18:09:10                 265\n",
       "2341399 2024-09-25 18:22:50                 265\n",
       "2345555 2024-09-25 18:35:37                 265\n",
       "2345556 2024-09-25 18:37:00                 265\n",
       "\n",
       "[2580093 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the Raw Data into Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the raw data and want to transform it into time series data just as we did before. This will be done with the **transform_raw_data_into_ts_data** function from **src.data**.\n",
    "\n",
    "The older data that we had is already transformed into time series data and loaded into our feature group. This is the new data that we now have to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:00<00:00, 316.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.data import transform_raw_data_into_ts_data\n",
    "\n",
    "ts_data = transform_raw_data_into_ts_data(rides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connect to Feature Store/Feature Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we will connect this new data to our feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1049751\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "# connect to project \n",
    "project = hopsworks.login(\n",
    "    project = config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value = config.HOPSWORKS_API_KEY\n",
    ")\n",
    "\n",
    "# get feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# connect to feature group\n",
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    "    description='Hourly time series data',\n",
    "    primary_key=['pickup_location_id', 'pickup_hour'],\n",
    "    event_time='pickup_hour'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ca3c093e4f4021a2054e3fccff5007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/178080 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: time_series_hourly_feature_group_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/1049751/jobs/named/time_series_hourly_feature_group_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x2bb214ad060>, None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert this data to the feature group\n",
    "feature_group.insert(ts_data, write_options={'wait_for_job':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automate Running Every Hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want this notebook to automatically run every hour. We will do this using GitHub actions. This is done by creating a **.github/workflows** folder which is placed in the parent directory. In this folder we will create a YAML file called **feature_workflows.yaml**. View the file to see the sytax. Help with the syntax can be found at the following links:\n",
    "\n",
    "+ https://docs.github.com/en/actions/use-cases-and-examples/creating-an-example-workflow\n",
    "\n",
    "+ https://spacelift.io/blog/github-actions-tutorial\n",
    "\n",
    "+ https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions\n",
    "\n",
    "+ https://docs.github.com/en/actions/writing-workflows/quickstart\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
