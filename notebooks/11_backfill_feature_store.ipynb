{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the feature store in the API calls we will need the project name and the API key. I am using CAPS for hopsworks variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project name for API call feature store\n",
    "HOPSWORKS_PROJECT_NAME = 'taxi_demand_rs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the API Key from the .env File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dotenv** library allows us to load variables from external files as environment variables. Environment variables means we can access them using the **os** module.\n",
    "\n",
    "A **.env** file is a plain text file that stores API keys and other sensitive information. This file is created within our project parent directory (not in the notebooks or src folders). We store it in this file because hardcoding an API key is a serious security violation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.paths import PARENT_DIR\n",
    "\n",
    "# specify the path where the file is\n",
    "load_dotenv(PARENT_DIR / '.env')\n",
    "\n",
    "HOPSWORKS_API_KEY = os.environ['HOPSWORKS_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Commiting to Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never commit the API key to GitHub repository. For this reason, we create a **gitignore** file. This file is located in the parent directory (not src, data, notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the **load_raw_data** function from the **src.data** script to load in raw data from 2022-today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from 2022 to 2024.\n",
      "File 2022-01 was already in local storage\n",
      "File 2022-02 was already in local storage\n",
      "File 2022-03 was already in local storage\n",
      "File 2022-04 was already in local storage\n",
      "File 2022-05 was already in local storage\n",
      "File 2022-06 was already in local storage\n",
      "File 2022-07 was already in local storage\n",
      "File 2022-08 was already in local storage\n",
      "File 2022-09 was already in local storage\n",
      "File 2022-10 was already in local storage\n",
      "File 2022-11 was already in local storage\n",
      "File 2022-12 was already in local storage\n",
      "File 2023-01 was already in local storage\n",
      "File 2023-02 was already in local storage\n",
      "File 2023-03 was already in local storage\n",
      "File 2023-04 was already in local storage\n",
      "File 2023-05 was already in local storage\n",
      "File 2023-06 was already in local storage\n",
      "File 2023-07 was already in local storage\n",
      "File 2023-08 was already in local storage\n",
      "File 2023-09 was already in local storage\n",
      "File 2023-10 was already in local storage\n",
      "File 2023-11 was already in local storage\n",
      "File 2023-12 was already in local storage\n",
      "File 2024-01 was already in local storage\n",
      "File 2024-02 was already in local storage\n",
      "File 2024-03 was already in local storage\n",
      "File 2024-04 was already in local storage\n",
      "File 2024-05 was already in local storage\n",
      "File 2024-06 was already in local storage\n",
      "File 2024-07 was already in local storage\n",
      "Downloading file 2024-08\n",
      "2024-08 file is not available\n",
      "Downloading file 2024-09\n",
      "2024-09 file is not available\n",
      "Downloading file 2024-10\n",
      "2024-10 file is not available\n",
      "Downloading file 2024-11\n",
      "2024-11 file is not available\n",
      "Downloading file 2024-12\n",
      "2024-12 file is not available\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from src.data import load_raw_data\n",
    "\n",
    "# use load_raw_data\n",
    "# starting year of data fetching\n",
    "start_year = 2022\n",
    "# ending year will be the current year\n",
    "end_year = datetime.now().year   \n",
    "print(f'Downloading files from {start_year} to {end_year}.')\n",
    "\n",
    "# set up an empty dataframe to be filled by function\n",
    "rides = pd.DataFrame()\n",
    "\n",
    "# loop to download all wanted data\n",
    "for year in range(start_year, end_year+1):\n",
    "    # download data for the year\n",
    "    rides_one_year = load_raw_data(year)\n",
    "\n",
    "    # append rows\n",
    "    rides = pd.concat([rides, rides_one_year])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101372930\n"
     ]
    }
   ],
   "source": [
    "print(len(rides))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the Data into Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data needs to be transformed into time series data. We can use the **transform_raw_data_into_ts_data** from the **data.py** module for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:05<00:00, 45.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.data import transform_raw_data_into_ts_data\n",
    "\n",
    "ts_data = transform_raw_data_into_ts_data(rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we connect to our project on Hopsworks. To do so, we will use the variable **HOPSWORKS_PROJECT_NAME** that we created at the top of this notebook along with the API key that we read in using **load_dotenv** called, **HOPSWORKS_API_KEY**. To connect to the project we import hopsworks then use **hopsworks.login()** with the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1049751\n"
     ]
    }
   ],
   "source": [
    "# connect to our project\n",
    "project = hopsworks.login(\n",
    "    project = HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value= HOPSWORKS_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have connected to the project we have the ability to use **project.get_feature_store()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# create feature store\n",
    "feature_store = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Features to the Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save data to the feature store we need to use **feature groups**. A feature group is just a table of features with predictive power for our model. Each group has a **primary key** and optionally an **event_time** column and a **partition key**. These are certain columns in our data. The partition key helps with queries such as a location id. \n",
    "\n",
    "To use a feature group we need to set the **name** and **version**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature group name and version\n",
    "FEATURE_GROUP_NAME = 'time_series_hourly_feature_group'\n",
    "FEATURE_GROUP_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the feature group\n",
    "feature_group = feature_store.get_or_create_feature_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
